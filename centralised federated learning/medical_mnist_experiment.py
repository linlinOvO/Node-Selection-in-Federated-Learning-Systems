# -*- coding: utf-8 -*-
"""Medical_MNIST_experiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16vtk_NrgG0aSKc6AxOLrMQZtelVY0N4P
"""

!pip install -q flwr[simulation] flwr_datasets[vision] torch torchvision matplotlib tqdm

# Standard library imports
import os
from collections import OrderedDict
from array import array

# Third-party library imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import math
from collections import defaultdict
import random
from PIL import Image
from tqdm import tqdm

# Type hinting imports
from typing import Callable, Dict, List, Optional, Tuple, Any  # For type annotations

# Google Colab specific imports
from google.colab import drive
drive.mount('/content/drive')

# PyTorch imports
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset, random_split, Subset

# torchvision imports
import torchvision.transforms as transforms  # For image transformations

# Flower (Federated Learning) imports
import flwr as fl  # Core Flower framework
from flwr.common import (  # Common Flower types and functions
    EvaluateIns,
    EvaluateRes,
    FitIns,
    FitRes,
    MetricsAggregationFn,
    NDArrays,
    Parameters,
    Scalar,
    Context,
    ndarrays_to_parameters,
    parameters_to_ndarrays,
)
from flwr.server.client_manager import ClientManager  # For managing clients on server-side
from flwr.server.client_proxy import ClientProxy  # For server-side client proxy management

# Device setup
DEVICE = torch.device("cpu")  # Try "cuda" to train on GPU
print(
    f"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}"
)

"""# Data preprocessing"""

input_path = '/content/drive/Othercomputers/My Mac/med-mnist'

# Gets the number of images in each folder
def count_images_in_folders(data_path):
    folder_counts = {}
    for folder in sorted(os.listdir(data_path)):
        folder_path = os.path.join(data_path, folder)
        if os.path.isdir(folder_path):
            num_images = len(os.listdir(folder_path))
            folder_counts[folder] = num_images
    return folder_counts

# Print the number of pictures in each folder
folder_counts = count_images_in_folders(input_path)
for folder, count in folder_counts.items():
    print(f"{folder}: {count} images")

img_path = '/content/drive/Othercomputers/My Mac/med-mnist/AbdomenCT/000000.jpeg'
image = Image.open(img_path)

image_array = np.array(image)
print(f'Image mode: {image.mode}')
print(f'Image shape: {image_array.shape}')

class MedicalMnistDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.image_paths = []
        self.labels = []
        self.classes = sorted(os.listdir(root_dir))

        for label, class_name in enumerate(self.classes):
            class_dir = os.path.join(root_dir, class_name)
            image_files = os.listdir(class_dir)
            for img_name in tqdm(image_files, total=len(image_files), desc=f'Loading {class_name}'):
                img_path = os.path.join(class_dir, img_name)
                self.image_paths.append(img_path)
                self.labels.append(label)

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        label = self.labels[idx]

        try:
            image = Image.open(img_path).convert('L')
        except Exception as e:
            print(f'Error loading image {img_path}: {e}')
            return None, None

        if self.transform:
            image = self.transform(image)

        return image, label

# 定义数据预处理的转换
transform = transforms.Compose([
    transforms.ToTensor(),  # 将图片转换为Tensor
    # transforms.Normalize(mean=[0.5], std=[0.5])  # 标准化，灰度图像只需一个通道
])

# 创建完整的数据集
dataset = MedicalMnistDataset(root_dir=input_path, transform=transform)

dataset[0][0].shape

# Randomly displaying 10 images
random_indices = random.sample(range(len(dataset)), 10)

plt.figure(figsize=(12, 8))

for i, idx in enumerate(random_indices):
    image, label = dataset[idx]
    image = transforms.ToPILImage()(image)

    plt.subplot(2, 5, i + 1)
    plt.imshow(image, cmap="gray")
    plt.title(f"Label: {label}")
    plt.axis('off')

plt.show()

train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

for images, labels in train_loader:
    print(f"Images batch shape: {images.size()}")
    print(f"Labels batch shape: {labels.size()}")
    break

"""# Generate balanced data and unbalanced data"""

class CustomDataset(Dataset):
    def __init__(self, X_data, y_data):
        self.X_data = X_data
        self.y_data = y_data

    def __len__(self):
        return len(self.X_data)

    def __getitem__(self, idx):
        x = torch.tensor(self.X_data[idx], dtype=torch.float32)
        y = torch.tensor(self.y_data[idx], dtype=torch.long)
        return x, y

def split_dataset_balanced(num_clients, train_dataset, test_dataset, batch_size=32):
    """
    Splits the given training and testing data into balanced subsets for a given number of clients
    and returns lists of DataLoader objects.
    """
    # Create datasets
    # train_dataset = CustomDataset(X_train, y_train)
    # test_dataset = CustomDataset(X_test, y_test)

    def create_balanced_subsets(dataset, num_clients):

        # Group indices by label
        label_to_indices = defaultdict(list)
        for idx in tqdm(range(len(dataset))):
            if idx % 10000 == 0:
                print(f"idx: {idx}")
            label = dataset[idx][1]
            label_to_indices[label].append(idx)

        # Find the minimum number of samples for any label
        min_label_count = min(len(indices) for indices in label_to_indices.values())

        # Determine the number of samples per client per label
        samples_per_client_per_label = min_label_count // num_clients

        # Create an array to hold indices for each client
        client_indices = [[] for _ in range(num_clients)]

        # Distribute indices for each label to each client
        for label, indices in label_to_indices.items():
            np.random.shuffle(indices)  # Shuffle indices to ensure randomness
            for i in range(num_clients):
                start_idx = i * samples_per_client_per_label
                end_idx = (i + 1) * samples_per_client_per_label
                client_indices[i].extend(indices[start_idx:end_idx])

        # Create subsets for each client
        subsets = [Subset(dataset, indices) for indices in client_indices]

        return subsets

    # Create balanced subsets for train and test datasets
    train_subsets = create_balanced_subsets(train_dataset, num_clients)
    test_subsets = create_balanced_subsets(test_dataset, num_clients)

    # Create DataLoaders for each subset
    train_loaders = [DataLoader(subset, batch_size=batch_size, shuffle=True) for subset in train_subsets]
    test_loaders = [DataLoader(subset, batch_size=batch_size, shuffle=False) for subset in test_subsets]

    return train_loaders, test_loaders

def create_unbalanced_loaders(train_loaders, test_loaders, label_counts, alpha):
    """
    Converts balanced train and test DataLoader objects into unbalanced DataLoader objects by redistributing the samples.

    """

    def create_unbalanced_loader(loader, p):
        label_to_indices = defaultdict(list)
        dataset = loader.dataset

        # Build the mapping from labels to indices
        for idx in tqdm(range(len(dataset))):
            _, label = dataset[idx]
            label_to_indices[label].append(idx)

        subset_indices = []
        for label, indices_list in label_to_indices.items():
            prob = p[label]
            for idx in indices_list:
                if np.random.rand() < prob:
                    subset_indices.append(idx)

        subset_dataset = Subset(dataset, subset_indices)
        subset_loader = DataLoader(subset_dataset, batch_size=loader.batch_size, shuffle=True)

        return subset_loader

    unbalanced_train_loaders = []
    unbalanced_test_loaders = []
    for i in range(len(train_loaders)):

        train_loader = train_loaders[i]
        test_loader = test_loaders[i]

        # Generate Dirichlet distribution to get probabilities
        p = np.random.dirichlet([alpha] * label_counts)

        # Normalize probabilities
        p /= np.max(p)

        unbalanced_train_loaders.append(create_unbalanced_loader(train_loader, p))
        unbalanced_test_loaders.append(create_unbalanced_loader(test_loader, p))

    return unbalanced_train_loaders, unbalanced_test_loaders

"""# Plot label distribution"""

def get_labels_from_subset(subset):
    labels = []
    for _, label in subset:
        labels.append(label)
    return labels

def plot_label_distribution(train_loaders):
    client_number = len(train_loaders)
    row = math.ceil(client_number / 5)
    fig, axs = plt.subplots(row, 5, figsize=(10, row * 2))  # Adjusted figsize for better visibility
    axs = axs.flatten()  # Flatten the array of axes for easier indexing

    for i, data_loader in enumerate(train_loaders):
        client_dataset = data_loader.dataset
        labels = get_labels_from_subset(client_dataset)
        label_counts = np.bincount(labels)
        # print(f"label_counts: {label_counts}")

        ax = axs[i]
        cmap = plt.cm.YlGnBu  # Choose colormap
        colors = cmap(np.linspace(0.1, 0.9, len(label_counts)))  # Generate colors
        ax.bar(range(len(label_counts)), label_counts, color=colors)
        ax.set_xlabel('Label')
        ax.set_ylabel('Count')
        ax.set_title(f'Client {i + 1}')

    # Hide any unused subplots
    for j in range(len(train_loaders), len(axs)):
        fig.delaxes(axs[j])

    # Adjust layout and display the plots
    plt.tight_layout()
    plt.show()

# plot_label_distribution(test_balanced_train_loaders)
# plot_label_distribution(test_unbalanced_train_loaders)

"""# CNN"""

def train(net, trainloader, epochs):
    """Train the network on the training set."""
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
    for _ in range(epochs):
        for images, labels in trainloader:
            images, labels = images.to(DEVICE), labels.to(DEVICE)
            optimizer.zero_grad()
            loss = criterion(net(images), labels)
            loss.backward()
            optimizer.step()

def test(net, testloader):
    """Validate the network on the entire test set."""
    criterion = torch.nn.CrossEntropyLoss()
    correct, total, loss = 0, 0, 0.0
    with torch.no_grad():
        for data in testloader:
            images, labels = data[0].to(DEVICE), data[1].to(DEVICE)
            outputs = net(images)
            loss += criterion(outputs, labels).item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    accuracy = correct / total
    return loss, accuracy

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, 5)        # 1 input channel (grayscale), 10 output channels, 5x5 kernel
        self.conv2 = nn.Conv2d(10, 32, 7)       # 10 input channels, 32 output channels, 7x7 kernel
        self.conv3 = nn.Conv2d(32, 64, 11)       # 32 input channels, 64 output channels, 11x11 kernel
        self.pool  = nn.MaxPool2d(2, 2)         # 2x2 pooling layer
        self.fc1   = nn.Linear(256, 128)        # Dense layer
        self.fc2   = nn.Linear(128, 64)         # Dense layer
        self.fc3   = nn.Linear(64, 6)          # 64 inputs, 6 outputs for the 6 classes
        # Steps:
            # Conv1: 64x64 -> 60x60 (64 - 5 + 1)
            # Pool:  60x60 -> 30x30 (60 / 2)
            # Conv2: 30x30 -> 24x24   (30 - 7 + 1)
            # Pool:  24x24   -> 12x12   (24 / 2)
            # Conv3: 12x12   -> 2x2   (12 - 11 + 1)
            # Flatn: 2x2   -> 1x256 (2 * 2 * 64)
            # Dense: 1x256 -> 1x128
            # Dense: 1x128 -> 1x64
            # Dense: 1x64  -> 1x6

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x))) # (30, 30, 10)
        x = self.pool(F.relu(self.conv2(x))) # (12, 12, 32)
        x = F.relu(self.conv3(x))            # (2, 2, 64)
        x = x.view(-1, 256)                  # Flatten
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

"""# Client"""

class Client(fl.client.NumPyClient):
    def __init__(self, net, train_loader, test_loader, num_examples):
        self.train_loader = train_loader
        self.test_loader = test_loader
        self.net = net
        self.num_examples = num_examples

    def get_parameters(self, config):
        return [val.cpu().numpy() for _, val in self.net.state_dict().items()]

    def set_parameters(self, parameters):
        params_dict = zip(self.net.state_dict().keys(), parameters)
        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})
        self.net.load_state_dict(state_dict, strict=True)

    def fit(self, parameters, config):
        self.set_parameters(parameters)
        train(self.net, self.train_loader, epochs=4)
        _, accuracy = test(self.net, self.test_loader)
        return self.get_parameters(config={}), self.num_examples["trainset"], {"accuracy": accuracy}

    def evaluate(self, parameters, config):
        self.set_parameters(parameters)
        loss, accuracy = test(self.net, self.test_loader)
        return float(loss), self.num_examples["testset"], {"accuracy": float(accuracy)}

def get_client_fn(train_loaders, test_loaders, num_examples):
    """Return a function to construct a client.

    """

    def client_fn(context: Context) -> Client:
        # print(f"context: {context}")
        cid = context.node_config["partition-id"]
        # print(f"cid: {cid}")

        train_loader = train_loaders[int(cid)]
        test_loader = test_loaders[int(cid)]
        net = Net().to(DEVICE)

        # print(f"cid: {int(cid)}")

        return Client(net, train_loader, test_loader, num_examples).to_client()

    return client_fn

"""# Strategy

## Method A (select all nodes each round)
"""

class MethodA(fl.server.strategy.FedAvg):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.accuracies = []
        self.losses = []
        self.accuracy_df = pd.DataFrame()

    def aggregate_evaluate(
        self,
        rnd: int,
        results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.EvaluateRes]],
        failures: List[BaseException]
    ) -> Tuple[float, Dict[str, Any]]:

        # Calculate accuracies for each client, create a dictionary and DataFrame for the round, and compute the mean accuracy
        round_accuracies = [result.metrics["accuracy"] for _, result in results]
        round_accuracy_dict = {f"Client {client_idx + 1}": accuracy for client_idx, accuracy in enumerate(round_accuracies)}
        round_df = pd.DataFrame(round_accuracy_dict, index=[rnd])
        accuracy = np.mean(round_accuracies)


        # Update accuracies, losses, and the accuracy DataFrame with the current round's data
        self.accuracies.append(accuracy)
        self.losses.append([result.loss for _, result in results])
        self.accuracy_df = pd.concat([self.accuracy_df, round_df])


        return super().aggregate_evaluate(rnd, results, failures)

    def plot_accuracies(self):
        plt.figure()
        plt.plot(range(1, len(self.accuracies) + 1), self.accuracies, marker='o')
        plt.title("Accuracy per Round")
        plt.xlabel("Round")
        plt.ylabel("Accuracy")
        plt.grid()
        plt.show()

    def plot_loss_per_client_per_round(self):
        num_rounds = len(self.losses)
        num_clients = len(self.losses[0])

        plt.figure()
        for client_idx in range(num_clients):
            plt.plot(range(1, num_rounds + 1), [self.losses[r][client_idx] for r in range(num_rounds)], marker='o', label=f'Client {client_idx + 1}')

        plt.title("Loss per Client per Round")
        plt.xlabel("Round")
        plt.ylabel("Loss")
        plt.legend()
        plt.grid()
        plt.show()

    def print_accuracy_matrix(self):
        print("Classification Accuracy Matrix:")
        print(self.accuracy_df)


    def save_accuracy_matrix(self, file_path):
        self.accuracy_df.to_csv(file_path, index_label='Round')

"""## Method B (Accuracy-Based Selection Strategy)"""

class MethodB(fl.server.strategy.FedAvg):
    def __init__(self, p, num_clients, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.p = p  # Fraction of clients to select
        self.num_clients = num_clients
        self.accuracies = []
        self.losses = []
        self.accuracy_df = pd.DataFrame()

        self.selected_clients = []  # To store selected clients
        self.accuracies = [None] * num_clients  # Store accuracies for each client
        self.probability_df = pd.DataFrame()

    def configure_fit(
        self, server_round: int, parameters: Parameters, client_manager: ClientManager
    ) -> List[Tuple[ClientProxy, FitIns]]:

        sample_size = int(self.p * self.num_clients)
        all_clients = client_manager.all()
        selected_clients_cids = [client.cid for client in self.selected_clients]

        if(server_round == 1):
            # Round 1: All nodes have p probability of selection
            selected_clients = client_manager.sample(num_clients=sample_size, min_num_clients=sample_size)

            # store round probabilities
            round_probabilities = {f"Client {cid + 1}": self.p for cid in range(self.num_clients)}

        else:
            # Round t: Nodes have selection probability based on accuracy from previous round
            selected_clients = []
            round_probabilities = {}

            for client_cid in range(self.num_clients):
                client_key = f"Client {client_cid + 1}"

                if str(client_cid) in selected_clients_cids:
                    last_round_accuracy = self.accuracy_df[client_key].iloc[-1]
                    prob = max(np.exp(-1.5 * last_round_accuracy), 0.1)
                else:
                    prob = self.p

                # store round probabilities
                round_probabilities[client_key] = prob

                if np.random.rand() < prob:
                    selected_clients.append(all_clients[str(client_cid)])

        # Ensure at least one client is selected
        if not selected_clients:
            selected_clients.append(all_clients[str(random.randint(0, self.num_clients - 1))])

        # update selected clients
        self.selected_clients = selected_clients

        # Update probability dataframe
        prob_df = pd.DataFrame(round_probabilities, index=[server_round])
        self.probability_df = pd.concat([self.probability_df, prob_df])

        fit_ins = FitIns(parameters, {})

        print(f"Serial number of the client selected to fit: {[client.cid for client in self.selected_clients]}")
        return [(client, fit_ins) for client in self.selected_clients]

    def configure_evaluate(
        self, server_round: int, parameters: Parameters, client_manager: ClientManager
    ) -> List[Tuple[ClientProxy, EvaluateIns]]:

        # Use only selected clients for evaluation
        evaluate_clients = self.selected_clients

        config = {}
        evaluate_ins = EvaluateIns(parameters, config)

        # Return client/config pairs for evaluation
        print(f"Serial number of the client selected to evaluate: {[client.cid for client in evaluate_clients]}")
        return [(client, evaluate_ins) for client in evaluate_clients]

    def aggregate_evaluate(
        self,
        rnd: int,
        results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.EvaluateRes]],
        failures: List[BaseException]
    ) -> Tuple[float, Dict[str, Any]]:

        # Initialize dictionaries to store evaluated client accuracy
        evaluated_client_dict = {}
        round_accuracy_dict = {}

        for client_proxy, evaluate_res in results:
            # Assume evaluate_res has an 'accuracy' attribute
            client_cid = int(client_proxy.cid)
            accuracy = evaluate_res.metrics["accuracy"]

            evaluated_client_dict[client_cid] = accuracy

        # Generate accuracy dict for all clients
        for client_cid in range(self.num_clients):
            client_key = f"Client {client_cid + 1}"
            if client_cid in evaluated_client_dict.keys():
                round_accuracy_dict[client_key] = evaluated_client_dict[client_cid]
            else:
                if(rnd == 1):
                    round_accuracy_dict[client_key] = float('NaN')
                else:
                    round_accuracy_dict[client_key] = self.accuracy_df[client_key].iloc[-1]

        round_accuracies = [result.metrics["accuracy"] for _, result in results]
        # round_accuracy_dict = {f"Client {client_idx + 1}": accuracy for client_idx, accuracy in enumerate(round_accuracies)}

        round_df = pd.DataFrame(round_accuracy_dict, index=[rnd])
        accuracy = np.mean(round_accuracies)

        self.accuracies.append(accuracy)
        self.losses.append([result.loss for _, result in results])
        self.accuracy_df = pd.concat([self.accuracy_df, round_df])

        return super().aggregate_evaluate(rnd, results, failures)

    def plot_accuracies(self):
        plt.figure()
        plt.plot(range(1, len(self.accuracies) + 1), self.accuracies, marker='o')
        plt.title("Accuracy per Round")
        plt.xlabel("Round")
        plt.ylabel("Accuracy")
        plt.grid()
        plt.show()

    def print_accuracy_matrix(self):
        print("Classification Accuracy Matrix:")
        print(self.accuracy_df)

    def save_accuracy_matrix(self, file_path):
        self.accuracy_df.to_csv(file_path, index_label='Round')

"""## Method C (Retain previously calculated selection probability if not selected.)"""

class MethodC(fl.server.strategy.FedAvg):
    def __init__(self, p, num_clients, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.p = p  # Fraction of clients to select
        self.num_clients = num_clients
        self.accuracies = [] # Store avg accuracies for each round
        self.losses = []
        self.accuracy_df = pd.DataFrame()

        self.selected_clients = []  # To store selected clients
        self.probability_df = pd.DataFrame()

    def configure_fit(
        self, server_round: int, parameters: Parameters, client_manager: ClientManager
    ) -> List[Tuple[ClientProxy, FitIns]]:

        sample_size = int(self.p * self.num_clients)
        all_clients = client_manager.all()
        selected_clients_cids = [client.cid for client in self.selected_clients]

        print(f"server_round: {server_round}")

        if(server_round == 1):
            # Round 1: All nodes have p probability of selection
            selected_clients = client_manager.sample(num_clients=sample_size, min_num_clients=sample_size)

            # store round probabilities
            round_probabilities = {f"Client {cid + 1}": self.p for cid in range(self.num_clients)}

        else:
            print(f"server_round: {server_round}")
            # Round t: Nodes have selection probability based on accuracy from previous round
            selected_clients = []
            round_probabilities = {}

            for client_cid in range(self.num_clients):
                client_key = f"Client {client_cid + 1}"

                if str(client_cid) in selected_clients_cids:
                    last_round_accuracy = self.accuracy_df[client_key].iloc[-1]
                    prob = np.exp(-1.5 * last_round_accuracy)
                else:
                    prob = self.probability_df[client_key].iloc[-1]

                # store round probabilities
                round_probabilities[client_key] = prob

                if np.random.rand() < prob:
                    selected_clients.append(all_clients[str(client_cid)])

        # Ensure at least one client is selected
        if not selected_clients:
            selected_clients.append(all_clients[str(random.randint(0, self.num_clients - 1))])

        # update selected clients
        self.selected_clients = selected_clients

        # Update probability dataframe
        prob_df = pd.DataFrame(round_probabilities, index=[server_round])
        self.probability_df = pd.concat([self.probability_df, prob_df])

        fit_ins = FitIns(parameters, {})

        print(f"Serial number of the client selected to fit: {[client.cid for client in self.selected_clients]}")
        return [(client, fit_ins) for client in self.selected_clients]

    def configure_evaluate(
        self, server_round: int, parameters: Parameters, client_manager: ClientManager
    ) -> List[Tuple[ClientProxy, EvaluateIns]]:

        # Use only selected clients for evaluation
        evaluate_clients = self.selected_clients

        config = {}
        evaluate_ins = EvaluateIns(parameters, config)

        # Return client/config pairs for evaluation
        print(f"Serial number of the client selected to evaluate: {[client.cid for client in evaluate_clients]}")
        return [(client, evaluate_ins) for client in evaluate_clients]

    def aggregate_evaluate(
        self,
        rnd: int,
        results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.EvaluateRes]],
        failures: List[BaseException]
    ) -> Tuple[float, Dict[str, Any]]:

        # Initialize dictionaries to store evaluated client accuracy
        evaluated_client_dict = {}
        round_accuracy_dict = {}

        for client_proxy, evaluate_res in results:
            # Assume evaluate_res has an 'accuracy' attribute
            client_cid = int(client_proxy.cid)
            accuracy = evaluate_res.metrics["accuracy"]

            evaluated_client_dict[client_cid] = accuracy

        # Generate accuracy dict for all clients
        for client_cid in range(self.num_clients):
            client_key = f"Client {client_cid + 1}"
            if client_cid in evaluated_client_dict.keys():
                round_accuracy_dict[client_key] = evaluated_client_dict[client_cid]
            else:
                if(rnd == 1):
                    round_accuracy_dict[client_key] = float('NaN')
                else:
                    round_accuracy_dict[client_key] = self.accuracy_df[client_key].iloc[-1]

        round_accuracies = [result.metrics["accuracy"] for _, result in results]
        # round_accuracy_dict = {f"Client {client_idx + 1}": accuracy for client_idx, accuracy in enumerate(round_accuracies)}

        round_df = pd.DataFrame(round_accuracy_dict, index=[rnd])
        accuracy = np.mean(round_accuracies)

        self.accuracies.append(accuracy)
        self.losses.append([result.loss for _, result in results])
        self.accuracy_df = pd.concat([self.accuracy_df, round_df])

        return super().aggregate_evaluate(rnd, results, failures)

    def plot_accuracies(self):
        plt.figure()
        plt.plot(range(1, len(self.accuracies) + 1), self.accuracies, marker='o')
        plt.title("Accuracy per Round")
        plt.xlabel("Round")
        plt.ylabel("Accuracy")
        plt.grid()
        plt.show()

    def print_accuracy_matrix(self):
        print("Classification Accuracy Matrix:")
        print(self.accuracy_df)

    def save_accuracy_matrix(self, file_path):
        self.accuracy_df.to_csv(file_path, index_label='Round')

"""## Method D"""

class MethodD(fl.server.strategy.FedAvg):
    def __init__(self, p, num_clients, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.p = p  # Fraction of clients to select
        self.num_clients = num_clients
        self.accuracies = []
        self.losses = []
        self.accuracy_df = pd.DataFrame()

        self.selected_clients = []  # To store selected clients
        self.probability_df = pd.DataFrame()
        self.availability_df = pd.DataFrame()  # DataFrame to store availability matrix

        self.state = {f"Client {i + 1}": "ON" for i in range(num_clients)}  # state for each clinet (all are 'ON' at the beginning)
        self.p1_dict = {f"Client {i + 1}": np.random.uniform() for i in range(num_clients)}  # Random p1 for each client
        self.p2_dict = {f"Client {i + 1}": np.random.uniform() for i in range(num_clients)}  # Random p2 for each client

        print("Initial p1 values:")
        for client, p1 in self.p1_dict.items():
            print(f"{client}: {p1}")

        print("\nInitial p2 values:")
        for client, p2 in self.p2_dict.items():
            print(f"{client}: {p2}")

    def configure_fit(
        self, server_round: int, parameters: Parameters, client_manager: ClientManager
    ) -> List[Tuple[ClientProxy, FitIns]]:

        sample_size = int(self.p * self.num_clients)
        all_clients = client_manager.all()
        selected_clients_cids = [client.cid for client in self.selected_clients]

        selected_clients = []
        round_probabilities = {}

        # check transition of availablity
        for client_cid in range(self.num_clients):
            client_key = f"Client {client_cid + 1}"
            if self.state[client_key] == "ON":
                if np.random.rand() <= self.p1_dict[client_key]:
                    self.state[client_key] = "OFF"
            elif self.state[client_key] == "OFF":
                if np.random.rand() <= self.p2_dict[client_key]:
                    self.state[client_key] = "ON"

        if(server_round == 1):
            # Round 1: All nodes have p probability of selection
            for client_cid in range(self.num_clients):
                client_key = f"Client {client_cid + 1}"

                if np.random.rand() <= self.p and self.state[client_key] == "ON":
                    selected_clients.append(all_clients[str(client_cid)])

                # store round probabilities
                round_probabilities[client_key] = self.p
        else:
            # Round t: Nodes have selection probability based on accuracy from previous round
            for client_cid in range(self.num_clients):
                client_key = f"Client {client_cid + 1}"

                if str(client_cid) in selected_clients_cids:
                    last_round_accuracy = self.accuracy_df[client_key].iloc[-1]
                    prob = np.exp(-1.5 * last_round_accuracy)
                else:
                    prob = self.probability_df[client_key].iloc[-1]

                # store round probabilities
                round_probabilities[client_key] = prob

                if np.random.rand() < prob and self.state[client_key] == "ON":
                    selected_clients.append(all_clients[str(client_cid)])

        # Ensure at least one client is selected
        if not selected_clients:
            selected_clients.append(all_clients[str(random.randint(0, self.num_clients - 1))])

        # update selected clients
        self.selected_clients = selected_clients

        # Update probability dataframe
        prob_df = pd.DataFrame(round_probabilities, index=[server_round])
        self.probability_df = pd.concat([self.probability_df, prob_df])

        # Update availability dataframe
        availability_dict = {}
        for client_key, state in self.state.items():
            availability_dict[client_key] = 1 if state == "ON" else 0

        avail_df = pd.DataFrame(availability_dict, index=[server_round])
        self.availability_df = pd.concat([self.availability_df, avail_df])

        # finish configure_fit
        fit_ins = FitIns(parameters, {})
        print(f"Serial number of the client selected to fit: {[client.cid for client in self.selected_clients]}")
        return [(client, fit_ins) for client in self.selected_clients]

    def configure_evaluate(
        self, server_round: int, parameters: Parameters, client_manager: ClientManager
    ) -> List[Tuple[ClientProxy, EvaluateIns]]:

        # Use only selected clients for evaluation
        evaluate_clients = self.selected_clients

        config = {}
        evaluate_ins = EvaluateIns(parameters, config)

        # Return client/config pairs for evaluation
        print(f"Serial number of the client selected to evaluate: {[client.cid for client in evaluate_clients]}")
        return [(client, evaluate_ins) for client in evaluate_clients]

    def aggregate_evaluate(
        self,
        rnd: int,
        results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.EvaluateRes]],
        failures: List[BaseException]
    ) -> Tuple[float, Dict[str, Any]]:

        # Initialize dictionaries to store evaluated client accuracy
        evaluated_client_dict = {}
        round_accuracy_dict = {}

        for client_proxy, evaluate_res in results:
            # Assume evaluate_res has an 'accuracy' attribute
            client_cid = int(client_proxy.cid)
            accuracy = evaluate_res.metrics["accuracy"]

            evaluated_client_dict[client_cid] = accuracy

        # Generate accuracy dict for all clients
        for client_cid in range(self.num_clients):
            client_key = f"Client {client_cid + 1}"
            if client_cid in evaluated_client_dict.keys():
                round_accuracy_dict[client_key] = evaluated_client_dict[client_cid]
            else:
                if(rnd == 1):
                    round_accuracy_dict[client_key] = float('NaN')
                else:
                    round_accuracy_dict[client_key] = self.accuracy_df[client_key].iloc[-1]

        round_accuracies = [result.metrics["accuracy"] for _, result in results]
        # round_accuracy_dict = {f"Client {client_idx + 1}": accuracy for client_idx, accuracy in enumerate(round_accuracies)}

        round_df = pd.DataFrame(round_accuracy_dict, index=[rnd])
        accuracy = np.mean(round_accuracies)

        self.accuracies.append(accuracy)
        self.losses.append([result.loss for _, result in results])
        self.accuracy_df = pd.concat([self.accuracy_df, round_df])

        return super().aggregate_evaluate(rnd, results, failures)

    def plot_accuracies(self):
        plt.figure()
        plt.plot(range(1, len(self.accuracies) + 1), self.accuracies, marker='o')
        plt.title("Accuracy per Round")
        plt.xlabel("Round")
        plt.ylabel("Accuracy")
        plt.grid()
        plt.show()

    def print_accuracy_matrix(self):
        print("Classification Accuracy Matrix:")
        print(self.accuracy_df)

    def save_accuracy_matrix(self, file_path):
        self.accuracy_df.to_csv(file_path, index_label='Round')

"""# Start Simulation

---


"""

NUM_CLIENTS = 20
ROUND = 30

def get_evaluate_server_fn(model, test_loader):
    def evaluate_fn(server_round, parameters, config):
        params_dict = zip(model.state_dict().keys(), parameters)
        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})
        model.load_state_dict(state_dict, strict=True)
        loss, accuracy = test(model, test_loader)
        return loss, {"accuracy": accuracy}
    return evaluate_fn

def start_simulation(train_loaders, test_loaders, strategy):
    train_set_length = sum(len(loader.dataset) for loader in train_loaders)
    test_set_length = sum(len(loader.dataset) for loader in test_loaders)
    num_examples = {"trainset": train_set_length, "testset": test_set_length}

    # Start simulation
    fl.simulation.start_simulation(
        client_fn=get_client_fn(train_loaders, test_loaders, num_examples),
        num_clients=NUM_CLIENTS,
        config=fl.server.ServerConfig(num_rounds=ROUND),
        strategy=strategy
    )


    strategy.plot_accuracies()
    strategy.print_accuracy_matrix()

import pickle

train_iid_loaders, test_iid_loaders = split_dataset_balanced(NUM_CLIENTS, train_dataset, test_dataset)

plot_label_distribution(train_iid_loaders)
plot_label_distribution(test_iid_loaders)

train_non_iid_loaders, test_non_iid_loaders = create_unbalanced_loaders(train_iid_loaders, test_iid_loaders, 6, alpha=0.5)

plot_label_distribution(train_non_iid_loaders)
plot_label_distribution(test_non_iid_loaders)

with open('/content/drive/MyDrive/Colab Notebooks/graduation project/Week5/medical-mnist/train_iid_loaders.pkl', 'wb') as f:
    pickle.dump(train_iid_loaders, f)

with open('/content/drive/MyDrive/Colab Notebooks/graduation project/Week5/medical-mnist/test_iid_loaders.pkl', 'wb') as f:
    pickle.dump(test_iid_loaders, f)

with open('/content/drive/MyDrive/Colab Notebooks/graduation project/Week5/medical-mnist/train_non_iid_loaders.pkl', 'wb') as f:
    pickle.dump(train_iid_loaders, f)

with open('/content/drive/MyDrive/Colab Notebooks/graduation project/Week5/medical-mnist/train_iid_loaders.pkl', 'wb') as f:
    pickle.dump(train_iid_loaders, f)

with open('/content/drive/MyDrive/Colab Notebooks/graduation project/Week5/medical-mnist/train_iid_loaders.pkl', 'rb') as f:
    train_iid_loaders = pickle.load(f)
with open('/content/drive/MyDrive/Colab Notebooks/graduation project/Week5/medical-mnist/test_iid_loaders.pkl', 'rb') as f:
    test_iid_loaders = pickle.load(f)
with open('/content/drive/MyDrive/Colab Notebooks/graduation project/Week5/medical-mnist/train_non_iid_loaders.pkl', 'rb') as f:
    train_non_iid_loaders = pickle.load(f)
with open('/content/drive/MyDrive/Colab Notebooks/graduation project/Week5/medical-mnist/test_non_iid_loaders.pkl', 'rb') as f:
    test_non_iid_loaders = pickle.load(f)

len(train_non_iid_loaders)

train_iid_loaders[0].dataset[0]

"""### Med-MNIST - Method A - iid data"""

med_mnist_iid_strategy_A = MethodA(evaluate_fn=get_evaluate_server_fn(Net().to(DEVICE), test_loader))

start_simulation(train_iid_loaders, test_iid_loaders, med_mnist_iid_strategy_A)
med_mnist_iid_strategy_A.save_accuracy_matrix(f'/content/drive/MyDrive/Colab Notebooks/graduation project/Week5/medical-mnist/iid_strategy_A_accuracy_matrix_clients={NUM_CLIENTS}_round={ROUND}.csv')

"""### Med-MNIST - Method A - non iid data"""

med_mnist_non_iid_strategy_A = MethodA(evaluate_fn=get_evaluate_server_fn(Net().to(DEVICE), test_loader))

start_simulation(train_non_iid_loaders, test_non_iid_loaders, med_mnist_non_iid_strategy_A)
med_mnist_non_iid_strategy_A.save_accuracy_matrix(f'/content/drive/MyDrive/Colab Notebooks/graduation project/Week5/medical-mnist/non_iid_strategy_A_accuracy_matrix_clients={NUM_CLIENTS}_round={ROUND}.csv')

med_mnist_iid_strategy_B = MethodB(0.5, NUM_CLIENTS, evaluate_fn=get_evaluate_server_fn(Net().to(DEVICE), test_loader))

start_simulation(train_iid_loaders, test_iid_loaders, med_mnist_iid_strategy_B)
med_mnist_iid_strategy_B.save_accuracy_matrix(f'/content/drive/MyDrive/Colab Notebooks/graduation project/Week5/medical-mnist/iid_strategy_B_accuracy_matrix_clients={NUM_CLIENTS}_round={ROUND}.csv')
med_mnist_iid_strategy_B.probability_df.to_csv(f'/content/drive/MyDrive/Colab Notebooks/graduation project/Week5/medical-mnist/iid_strategy_B_probability_matrix_clients={NUM_CLIENTS}_round={ROUND}.csv')

med_mnist_non_iid_strategy_B = MethodB(0.5, NUM_CLIENTS, evaluate_fn=get_evaluate_server_fn(Net().to(DEVICE), test_loader))

start_simulation(train_iid_loaders, test_iid_loaders, med_mnist_non_iid_strategy_B)
med_mnist_non_iid_strategy_B.save_accuracy_matrix(f'/content/drive/MyDrive/Colab Notebooks/graduation project/Week5/medical-mnist/non_iid_strategy_B_accuracy_matrix_clients={NUM_CLIENTS}_round={ROUND}.csv')
med_mnist_non_iid_strategy_B.probability_df.to_csv(f'/content/drive/MyDrive/Colab Notebooks/graduation project/Week5/medical-mnist/non_iid_strategy_B_probability_matrix_clients={NUM_CLIENTS}_round={ROUND}.csv')

med_mnist_iid_strategy_C = MethodC(0.5, NUM_CLIENTS, evaluate_fn=get_evaluate_server_fn(Net().to(DEVICE), test_loader))

start_simulation(train_iid_loaders, test_iid_loaders, med_mnist_iid_strategy_C)
med_mnist_iid_strategy_C.save_accuracy_matrix(f'/content/drive/MyDrive/Colab Notebooks/graduation project/Week5/medical-mnist/iid_strategy_C_accuracy_matrix_clients={NUM_CLIENTS}_round={ROUND}.csv')
med_mnist_iid_strategy_C.probability_df.to_csv(f'/content/drive/MyDrive/Colab Notebooks/graduation project/Week5/medical-mnist/iid_strategy_C_probability_matrix_clients={NUM_CLIENTS}_round={ROUND}.csv')

med_mnist_non_iid_strategy_C = MethodC(0.5, NUM_CLIENTS, evaluate_fn=get_evaluate_server_fn(Net().to(DEVICE), test_loader))

start_simulation(train_non_iid_loaders, test_non_iid_loaders, med_mnist_non_iid_strategy_C)
med_mnist_non_iid_strategy_C.save_accuracy_matrix(f'/content/drive/MyDrive/Colab Notebooks/graduation project/Week5/medical-mnist/non_iid_strategy_C_accuracy_matrix_clients={NUM_CLIENTS}_round={ROUND}.csv')
med_mnist_non_iid_strategy_C.probability_df.to_csv(f'/content/drive/MyDrive/Colab Notebooks/graduation project/Week5/medical-mnist/non_iid_strategy_C_probability_matrix_clients={NUM_CLIENTS}_round={ROUND}.csv')

med_mnist_iid_strategy_D = MethodD(0.5, NUM_CLIENTS, evaluate_fn=get_evaluate_server_fn(Net().to(DEVICE), test_loader))

start_simulation(train_iid_loaders, test_iid_loaders, med_mnist_iid_strategy_D)
med_mnist_iid_strategy_D.save_accuracy_matrix(f'/content/drive/MyDrive/Colab Notebooks/graduation project/Week5/medical-mnist/iid_strategy_D_accuracy_matrix_clients={NUM_CLIENTS}_round={ROUND}.csv')
med_mnist_iid_strategy_D.probability_df.to_csv(f'/content/drive/MyDrive/Colab Notebooks/graduation project/Week5/medical-mnist/iid_strategy_D_probability_matrix_clients={NUM_CLIENTS}_round={ROUND}.csv')

med_mnist_non_iid_strategy_D = MethodC(0.5, NUM_CLIENTS, evaluate_fn=get_evaluate_server_fn(Net().to(DEVICE), test_loader))

start_simulation(train_non_iid_loaders, test_non_iid_loaders, med_mnist_non_iid_strategy_D)
med_mnist_non_iid_strategy_D.save_accuracy_matrix(f'/content/drive/MyDrive/Colab Notebooks/graduation project/Week5/medical-mnist/non_iid_strategy_D_accuracy_matrix_clients={NUM_CLIENTS}_round={ROUND}.csv')
med_mnist_non_iid_strategy_D.probability_df.to_csv(f'/content/drive/MyDrive/Colab Notebooks/graduation project/Week5/medical-mnist/non_iid_strategy_D_probability_matrix_clients={NUM_CLIENTS}_round={ROUND}.csv')